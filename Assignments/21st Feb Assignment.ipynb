{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35d8416-1b06-45dd-85f5-7e9ef007dcbf",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "Web scraping is the process of collecting structured web data in an automated manner. It’s also widely known as web data extraction or web data scraping.\n",
    "\n",
    "\n",
    "In general, web scraping is used by people and businesses who want to make use of publicly available web data to generate valuable insights and make smarter decisions. In such a situation, copying and pasting will not work! And that’s when you’ll need to use Web Scraping. Unlike the long and mind-numbing process of manually getting data, Web scraping uses intelligence automation methods to get thousands or even millions of data sets in a smaller amount of time.\n",
    "\n",
    "\n",
    "Below are three of the most common areas where businesses scraped data to accelerate their growth:\n",
    "\n",
    "1. Price intelligence\n",
    "2. Market research\n",
    "3. Data-driven products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8ef43-149c-44d0-9ff3-be37ed6cb3de",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "1. Web Scraping Libraries: Web scraping libraries are software packages that provide pre-built functions and tools for web scraping tasks (Figure 1). These libraries simplify the process of navigating web pages, parsing HTML data, and locating elements to extract.\n",
    "\n",
    "2. Web Scraping Tools: A web scraping tool is a software or program that automatically gathers data from web sources. \n",
    "\n",
    "3. Web Scraping APIs: Web scraping APIs enable developers to access and extract relevant data from websites. Websites can provide web scraping APIs, such as Twitter API, Amazon API, and Facebook API.\n",
    "\n",
    "4. Optical Character Recognition (OCR): Optical Character Recognition (OCR) is a technology that allows users to extract text data from images (screen scraping) or scanned documents on web pages.\n",
    "\n",
    "5. Headless Browsers: Headless browsers such as PhantomJS, Puppeteer, or Selenium enable users to collect web data in a headless mode, meaning that it runs without a graphical user interface.\n",
    "\n",
    "6. HTML Parsing: HTML parsing is another technique used to extract data from HTML code automatically.\n",
    "\n",
    "7. DOM Parsing: DOM parsing allows you to parse HTML or XML documents into their corresponding Document Object Model (DOM) representation. DOM Parser is part of the W3C standard that provides methods to navigate the DOM tree and extract desired information from it, such as text or attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be856b-bfd8-41fa-9697-e524ece90b7e",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup is a Python library for parsing structured data. It allows you to interact with HTML in a similar way to how you interact with a web page using developer tools.\n",
    "\n",
    "The library exposes a couple of intuitive functions you can use to explore the HTML you received. \n",
    "\n",
    "<b><u>Code:</u></b> \n",
    "\n",
    "<pre>\n",
    "URL = \"https:/flipkart.com\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "</pre>\n",
    "\n",
    "\n",
    "1. In the above code, BeautifulSoup() creating a soup obejct by taking \"page.content\" (basically the html dump from the page of the mentioned URL) as a first argument and the \"html.parser\" as second argument makes sure that you use the appropriate parser for HTML content.\n",
    "\n",
    "\n",
    "2. In an HTML web page, every element can have an id attribute assigned which unique identify a HTML element. Beautiful Soup allows you to find that specific HTML element by its ID shown as below.\n",
    "\n",
    "<pre>\n",
    "results = soup.find(id=\"ResultsContainer\")\n",
    "</pre>\n",
    "\n",
    "\n",
    "3. For easier viewing, you can prettify any Beautiful Soup object when you print it out. If you call .prettify() on the results variable that you just assigned above, then you’ll see all the HTML contained within the element.\n",
    "\n",
    "<pre>\n",
    "print(results.prettify())\n",
    "</pre>\n",
    "\n",
    "\n",
    "4. You can find all the HTML elements by their class name using find_all() function on a soup object. The below code retunrs an iterable (list) containing all the \"div\" element with class \"card-content\".\n",
    "\n",
    "<pre>\n",
    "job_elements = results.find_all(\"div\", class_=\"card-content\")\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4b832-8b2e-4683-8d8d-3944152c50ce",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "1. Flask is a lightweight framework, so it is easy for a beginner to get started with web scrapping.\n",
    "2. It's very easy to create API in Flask and work with requests and BeautifulSoup library to handle web data.\n",
    "3. It does not have any compatibility issues, so any type of database can be used to store the webdata, be it SQL based database or a No-SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688b517-3bf0-4d87-af08-acae94dad73b",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "AWS services used in the project is: Elastic Beanstalk and Code Pipeline.\n",
    "\n",
    "<b>Process:</b>\n",
    "1. From IDE(PWSKILL LAB VS CODE) to github repository.\n",
    "2. create a AWS Account.\n",
    "3. create a Elastic Beanstalk instance.\n",
    "4. Create a Code Pipeline to build a connection between you github repository where you have the code to Beanstalk Instance.\n",
    "\n",
    "<b><u>AWS Elastic Beanstalk:</b></u> This AWS service is used to create an application on AWS Cloud Server. So, this is basically a computational instance which will provide the CPU and memory for the code to be executed on demand.\n",
    "\n",
    "\n",
    "<b><u>Code Pipeline:</u></b> AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production. AWS CodePipeline then builds, tests, and deploys your application according to the defined workflow every time there is a code change. \n",
    "\n",
    "In our project, we first deployed our code to the github repository. Then using AWS code pipeline, we build a delivery service between the github code repository and the AWS Elastic Beanstalk application to make it available in the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99e84e-97db-44a5-91b4-51512abff2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca0017-a8a8-454d-a343-8c45f56782e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
