{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is boosting in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a powerful ensemble learning method in machine learning, specifically designed to improve the accuracy of predictive models by combining multiple weak learners—models that perform only slightly better than random guessing—into a single, strong learner. \n",
    "\n",
    "The essence of boosting lies in the iterative process where each weak learner is trained to correct the errors of its predecessor, gradually enhancing the overall model's performance. By focusing on the mistakes made by earlier models, boosting turns a collection of weak learners into a more accurate model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and limitations of using boosting techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Advantages of Boosting:</b>\n",
    "\n",
    "Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model. \n",
    "Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly. \n",
    "Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified \n",
    "Better Interpretability – Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes.\n",
    "\n",
    "<b>Limitations of Boosting Algorithms:</b>\n",
    "\n",
    "Boosting algorithms also have some disadvantages these are:\n",
    "\n",
    "* Boosting Algorithms are vulnerable to the outliers \n",
    "* It is difficult to use boosting algorithms for Real-Time applications.\n",
    "* It is computationally expensive for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Explain how boosting works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting transforms weak learners into one unified, strong learner through a systematic process that focuses on reducing errors in sequential model training. The steps involved include:\n",
    "\n",
    "1. <b>Select Initial Weights</b>: Assign initial weights to all data points to indicate their importance in the learning process.\n",
    "2. <b>Train Sequentially</b>: Train the first weak learner on the data. After evaluating its performance, increase the weights of misclassified instances. This makes the next weak learner focus more on the harder cases.\n",
    "3. <b>Iterate the Process</b>: Repeat the process of adjusting weights and training subsequent learners. Each new model focuses on the weaknesses of the ensemble thus far.\n",
    "4. <b>Combine the Results</b>: Aggregate the predictions of all weak learners to form the final output. The aggregation is typically weighted, where more accurate learners have more influence.\n",
    "\n",
    "This method effectively minimizes errors by focusing more intensively on difficult cases in the training data, resulting in a strong predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. What are the different types of boosting algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at some of the most well-known boosting algorithms. \n",
    "\n",
    "<b>AdaBoost (Adaptive Boosting)</b>:\n",
    "\n",
    "    AdaBoost is one of the first boosting algorithms. It focuses on reweighting the training examples each time a learner is added, putting more emphasis on the incorrectly classified instances. AdaBoost is particularly effective for binary classification problems. Read our AdaBoost Classifier in Python tutorial to learn more. \n",
    "\n",
    "<b>Gradient Boosting</b>:\n",
    "\n",
    "    Gradient boosting builds models sequentially and corrects errors along the way. It uses a gradient descent algorithm to minimize the loss when adding new models. This method is flexible and can be used for both regression and classification problems. Our tutorial, A Guide to The Gradient Boosting Algorithm, describes this process in detail. \n",
    "\n",
    "<b>XGBoost (Extreme Gradient Boosting)</b>\n",
    "\n",
    "    XGBoost is an optimized distributed gradient boosting library and the go-to method for many competition winners on Kaggle. It is designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework, offering a scalable and accurate solution to many practical data issues. For a more detailed study, consider reviewing our Using XGBoost in Python tutorial and taking our dedicated course: Extreme Gradient Boosting with XGBoost.\n",
    "\n",
    "<b>Ensemble Methods</b>:\n",
    "\n",
    "    Boosting belongs to the larger group of ensemble methods. Ensemble methods are an approach in machine learning that combines multiple models to produce more accurate predictions than any single model could typically achieve alone. These techniques work by utilizing the diversity of different models, each with its own strengths and limitations, to create a collective decision-making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Hyperparameters\n",
    "Since we are talking about Gradient Boosting Hyperparameters let us see what different Hyperparameters are there that can be tuned.\n",
    "\n",
    "1. n_estimators: Defines the number of boosting iterations (trees) to be added. More estimators usually lead to better performance, but also increase the risk of overfitting.\n",
    "\n",
    "* By default: n_estimators=100\n",
    "* n_estimators=100 means the model uses 100 decision trees to make predictions.\n",
    "\n",
    "2. learning_rate: Controls the contribution of each tree to the final prediction. A smaller value makes the model more robust but requires more estimators to achieve high performance.\n",
    "\n",
    "* By default: learning_rate=0.1\n",
    "* learning_rate=0.1 means that each additional tree will have a 10% influence on the overall prediction\n",
    "\n",
    "3. max_depth: Specifies the maximum depth of each individual tree. Shallow trees might underfit while deeper trees can overfit. It's essential to find the right depth.\n",
    "\n",
    "* By default: max_depth=None\n",
    "\n",
    "4. min_samples_split: Defines the minimum number of samples required to split an internal node. Increasing this value helps control overfitting by preventing the model from learning overly specific patterns.\n",
    "\n",
    "* By default: min_samples_split=2\n",
    "* min_samples_split=2 means that every node in the tree will have at least 2 samples before being split\n",
    "\n",
    "5. subsample: Specifies the fraction of samples to be used for fitting each individual tree.\n",
    "\n",
    "* By default: subsample=1.0\n",
    "* subsample=1.0 means that the model uses the entire dataset for each tree but using a fraction like 0.8 helps prevent overfitting by introducing more randomness.\n",
    "\n",
    "6. colsample_bytree: Defines the fraction of features to be randomly sampled for building each tree. It is another method for controlling overfitting.\n",
    "\n",
    "* By default: colsample_bytree=1.0\n",
    "* colsample_bytree=1.0 means that the model uses all the available features to build each tree.\n",
    "\n",
    "7. min_samples_leaf: Defines the minimum number of samples required to be at a leaf node. Increasing this value can reduce overfitting by preventing the tree from learning overly specific patterns.\n",
    "\n",
    "* By default: min_samples_leaf=1\n",
    "* min_samples_leaf=1 means that the tree is allowed to create leaf nodes with a single sample.\n",
    "\n",
    "8. max_features: Specifies the number of features to consider when looking for the best split.\n",
    "\n",
    "* By default: max_features=None\n",
    "* max_features=None means all features are considered for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting algorithms combine multiple weak learners in a sequential method, iteratively improving upon observations. This approach can help to reduce high bias, commonly seen in shallow decision trees and logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is an ensemble machine learning algorithm that can be used in a wide variety of classification and regression tasks. It is a supervised learning algorithm that is used to classify data by combining multiple weak or base learners (e.g., decision trees) into a strong learner. AdaBoost works by weighting the instances in the training dataset based on the accuracy of previous classifications.\n",
    "\n",
    "\n",
    "<b>How Does The AdaBoost Work?</b>:\n",
    "\n",
    "We can understand the working of the AdaBoost algorithm in step by step manner as going deep into the work, we can see there are multiple basic steps which this algorithm follows. Let’s take a look at these steps.\n",
    "\n",
    "Step 1: When the algorithm is given data, it starts by Assigning equal weights to all training examples in the dataset. These weights represent the importance of each sample during the training process.\n",
    "\n",
    "Step 2: Here, this algorithm iterates with a few algorithms for a specified number of iterations (or until a stopping criterion is met). The algorithm trains a weak classifier on the training data. Here the weak classifier can be considered a model that performs slightly better than random guessing, such as a decision stump (a one-level decision tree).\n",
    "\n",
    "Step 3: During each iteration, the algorithm trains the weak classifier on given training data with the current sample weights. The weak classifier aims to minimize the classification error, weighted by the sample weights.\n",
    "\n",
    "Step 4: After training the weak classifier, the algorithm calculates classifier weight based on the errors of the weak classifier. A weak classifier with a lower error receives a higher weight.\n",
    "\n",
    "Step 5: Once the calculation of weight completes, the algorithm updates sample weights, and the algorithm gives assigns higher weights to misclassified examples so that more importance in subsequent iterations can be given.\n",
    "\n",
    "Step 6: After updating the sample weights, they are normalized so that they sum up to 1 and Combine the predictions of all weak classifiers using a weighted majority vote. The weights of the weak classifiers are considered when making the final prediction.\n",
    "\n",
    "Step 7: Finally, Steps 2–5 are repeated for the specified number of iterations (or until the stopping criterion is met), with the sample weights updated at each iteration. The final prediction is obtained by aggregating the predictions of all weak classifiers based on their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential loss function is defined as follows:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We can see that as always it takes in two arguments, the target $y$, and $f(x)$ which is the model. We can see that when $y$ and $f(x)$ are the same sign, the output approaches 0. When $y$ and $f(x)$ are opposite signs, the output approaches infinity. This means we still have the same asymptotic effect of the cross entropy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a weak learner misclassifies a data point, its weight is increased, making the algorithm focus more on these challenging cases in subsequent rounds. Weak Learners: These are simple models, like decision stumps, that perform slightly better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate depends highly upon the number of n_estimators. By default, it is set to 1 but it can be increased or decreased depending on the estimators used. Generally, for a large number of n_estimators, we use a smaller value of learning rate. For example when our weak classifier has the chances of right predictions just slightly more than random guess then the learning rate is 0.5. It is common to use a smaller value of learning rate ranging between 0 and 1, like 0.1,0.001 because otherwise, it gives rise to the problem of overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
